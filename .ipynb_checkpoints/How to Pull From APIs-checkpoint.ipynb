{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling From APIs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import these libraries \n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need the requests library. In order to pull from APIs you can either use the Requests our URLIB2 library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step is to find the url\n",
    "url = \"http://www.reddit.com/r/weather.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we need to create a header\n",
    "# Headers might vary based upon APis, most will require a password, but similar concept.\n",
    "res = requests.get(url, headers={'User-agent': 'Marc Bot 0.1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure to\n",
    "res.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.reddit.com/r/weather.json'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 0: https://www.reddit.com/r/weather/top.json?t=all\n",
      "<Response [200]>\n",
      "Page 1: https://www.reddit.com/r/weather/top.json?t=all&after=t3_bulkjc\n",
      "<Response [200]>\n",
      "Page 2: https://www.reddit.com/r/weather/top.json?t=all&after=t3_bgj27g\n",
      "<Response [200]>\n",
      "Page 3: https://www.reddit.com/r/weather/top.json?t=all&after=t3_963sed\n",
      "<Response [200]>\n",
      "Page 4: https://www.reddit.com/r/weather/top.json?t=all&after=t3_c0ebv9\n",
      "<Response [200]>\n",
      "Page 5: https://www.reddit.com/r/weather/top.json?t=all&after=t3_8fb2aa\n",
      "<Response [200]>\n",
      "Page 6: https://www.reddit.com/r/weather/top.json?t=all&after=t3_9dtap9\n",
      "<Response [200]>\n",
      "Page 7: https://www.reddit.com/r/weather/top.json?t=all&after=t3_b4uohq\n",
      "<Response [200]>\n",
      "Page 8: https://www.reddit.com/r/weather/top.json?t=all&after=t3_6ycido\n",
      "<Response [200]>\n",
      "Page 9: https://www.reddit.com/r/weather/top.json?t=all&after=t3_89eu42\n",
      "<Response [200]>\n",
      "Page 10: https://www.reddit.com/r/weather/top.json?t=all&after=t3_aoidku\n",
      "<Response [200]>\n",
      "Page 11: https://www.reddit.com/r/weather/top.json?t=all&after=t3_1g5ojy\n",
      "<Response [200]>\n",
      "Page 12: https://www.reddit.com/r/weather/top.json?t=all&after=t3_aq0u9i\n",
      "<Response [200]>\n",
      "Page 13: https://www.reddit.com/r/weather/top.json?t=all&after=t3_8zboe3\n",
      "<Response [200]>\n",
      "Page 14: https://www.reddit.com/r/weather/top.json?t=all&after=t3_c0lnkv\n",
      "<Response [200]>\n",
      "Page 15: https://www.reddit.com/r/weather/top.json?t=all&after=t3_bu2ktj\n",
      "<Response [200]>\n",
      "Page 16: https://www.reddit.com/r/weather/top.json?t=all&after=t3_5n4kg9\n",
      "<Response [200]>\n",
      "Page 17: https://www.reddit.com/r/weather/top.json?t=all&after=t3_91n03u\n",
      "<Response [200]>\n",
      "Page 18: https://www.reddit.com/r/weather/top.json?t=all&after=t3_9copkp\n",
      "<Response [200]>\n",
      "Page 19: https://www.reddit.com/r/weather/top.json?t=all&after=t3_5aq7sh\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "# next step will be to build a for loop to get posts from reddit\n",
    "posts = []\n",
    "url = 'https://www.reddit.com/r/weather/top.json?t=all'\n",
    "after = None\n",
    "\n",
    "#Step 1: build the loop and print URL\n",
    "for _ in range(20):\n",
    "    if after == None:\n",
    "        current_url = url\n",
    "    else:\n",
    "        current_url = url + '&after=' + after\n",
    "        \n",
    "    print(f'Page {_}:', current_url)\n",
    "    \n",
    "    \n",
    "    #Step 2a: make the requests and handle status code. Add time.sleep now (2.b)\n",
    "    res = requests.get(current_url, headers={'User-agent': 'Marc Bot 0.1'})\n",
    "\n",
    "    if res.status_code != 200:\n",
    "        print('Status error', res.status_code)\n",
    "        break\n",
    "    else:\n",
    "        print(res)\n",
    "\n",
    "        \n",
    "    #Step 3: Actually deal with the data   \n",
    "    current_dict = res.json()\n",
    "    current_posts = [p['data'] for p in current_dict['data']['children']]\n",
    "    posts.extend(current_posts)\n",
    "\n",
    "    after = current_dict['data']['after']\n",
    "\n",
    "    #Step 2b:\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a step back!\n",
    "In order to make it all work we need to work through that messy JSON. The key here is to find the right dictionary keys. Not all JSONS are created equally, but all have similar concepts. The goal is to narrow down that incredible mess!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy and pase that link into a web browser. http://www.reddit.com/r/weather.json\n",
    "Look how messy that is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['kind', 'data'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['modhash', 'dist', 'children', 'after', 'before'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dict['data'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['kind', 'data'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dict['data']['children'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(current_dict['data']['children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kind': 't3',\n",
       " 'data': {'approved_at_utc': None,\n",
       "  'subreddit': 'weather',\n",
       "  'selftext': '',\n",
       "  'author_fullname': 't2_2luw301h',\n",
       "  'saved': False,\n",
       "  'mod_reason_title': None,\n",
       "  'gilded': 0,\n",
       "  'clicked': False,\n",
       "  'title': 'Land-icane over us in North Dakota back in November 2018',\n",
       "  'link_flair_richtext': [],\n",
       "  'subreddit_name_prefixed': 'r/weather',\n",
       "  'hidden': False,\n",
       "  'pwls': 6,\n",
       "  'link_flair_css_class': None,\n",
       "  'downs': 0,\n",
       "  'thumbnail_height': 140,\n",
       "  'hide_score': False,\n",
       "  'name': 't3_awb0vh',\n",
       "  'quarantine': False,\n",
       "  'link_flair_text_color': 'dark',\n",
       "  'author_flair_background_color': None,\n",
       "  'subreddit_type': 'public',\n",
       "  'ups': 194,\n",
       "  'total_awards_received': 0,\n",
       "  'media_embed': {},\n",
       "  'thumbnail_width': 140,\n",
       "  'author_flair_template_id': None,\n",
       "  'is_original_content': False,\n",
       "  'user_reports': [],\n",
       "  'secure_media': {'reddit_video': {'fallback_url': 'https://v.redd.it/7c4357xqalj21/DASH_1080?source=fallback',\n",
       "    'height': 1080,\n",
       "    'width': 608,\n",
       "    'scrubber_media_url': 'https://v.redd.it/7c4357xqalj21/DASH_240',\n",
       "    'dash_url': 'https://v.redd.it/7c4357xqalj21/DASHPlaylist.mpd',\n",
       "    'duration': 7,\n",
       "    'hls_url': 'https://v.redd.it/7c4357xqalj21/HLSPlaylist.m3u8',\n",
       "    'is_gif': True,\n",
       "    'transcoding_status': 'completed'}},\n",
       "  'is_reddit_media_domain': True,\n",
       "  'is_meta': False,\n",
       "  'category': None,\n",
       "  'secure_media_embed': {},\n",
       "  'link_flair_text': None,\n",
       "  'can_mod_post': False,\n",
       "  'score': 194,\n",
       "  'approved_by': None,\n",
       "  'thumbnail': 'https://b.thumbs.redditmedia.com/U41pnhmhw4s_Ma7iV8Koza-JlVpXMOSDHdZzDr27Qqs.jpg',\n",
       "  'edited': False,\n",
       "  'author_flair_css_class': None,\n",
       "  'author_flair_richtext': [],\n",
       "  'gildings': {},\n",
       "  'post_hint': 'hosted:video',\n",
       "  'content_categories': None,\n",
       "  'is_self': False,\n",
       "  'mod_note': None,\n",
       "  'created': 1551510763.0,\n",
       "  'link_flair_type': 'text',\n",
       "  'wls': 6,\n",
       "  'banned_by': None,\n",
       "  'author_flair_type': 'text',\n",
       "  'domain': 'v.redd.it',\n",
       "  'selftext_html': None,\n",
       "  'likes': None,\n",
       "  'suggested_sort': None,\n",
       "  'banned_at_utc': None,\n",
       "  'view_count': None,\n",
       "  'archived': False,\n",
       "  'no_follow': False,\n",
       "  'is_crosspostable': False,\n",
       "  'pinned': False,\n",
       "  'over_18': False,\n",
       "  'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/8fPBogW2s8T4sjxF4Rmh5cj-K4mx0SQmYNO6i3JTJhE.png?format=pjpg&amp;auto=webp&amp;s=ac558924181b9e6ffeb7e95af6e381c9a9f1d5b3',\n",
       "      'width': 1080,\n",
       "      'height': 1920},\n",
       "     'resolutions': [{'url': 'https://external-preview.redd.it/8fPBogW2s8T4sjxF4Rmh5cj-K4mx0SQmYNO6i3JTJhE.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5704017d0bc5df83f91de1d0ce6b82ac10d77b46',\n",
       "       'width': 108,\n",
       "       'height': 192},\n",
       "      {'url': 'https://external-preview.redd.it/8fPBogW2s8T4sjxF4Rmh5cj-K4mx0SQmYNO6i3JTJhE.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d468de4f5dd812f4df3765928532763c23906f2d',\n",
       "       'width': 216,\n",
       "       'height': 384},\n",
       "      {'url': 'https://external-preview.redd.it/8fPBogW2s8T4sjxF4Rmh5cj-K4mx0SQmYNO6i3JTJhE.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=303c19f3b67299565e42a01e0d6912f3df8198db',\n",
       "       'width': 320,\n",
       "       'height': 568},\n",
       "      {'url': 'https://external-preview.redd.it/8fPBogW2s8T4sjxF4Rmh5cj-K4mx0SQmYNO6i3JTJhE.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=52737ac2cb1a0e6e361ada895d56a98dcb580ca6',\n",
       "       'width': 640,\n",
       "       'height': 1137},\n",
       "      {'url': 'https://external-preview.redd.it/8fPBogW2s8T4sjxF4Rmh5cj-K4mx0SQmYNO6i3JTJhE.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=780a5fc04a3d58ce731b08af5235dcc8cea036b3',\n",
       "       'width': 960,\n",
       "       'height': 1706},\n",
       "      {'url': 'https://external-preview.redd.it/8fPBogW2s8T4sjxF4Rmh5cj-K4mx0SQmYNO6i3JTJhE.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=697b12d6f4991c93a74329bd2b5303b9db6e0cd0',\n",
       "       'width': 1080,\n",
       "       'height': 1920}],\n",
       "     'variants': {},\n",
       "     'id': '-yfhgTHVZvC2Fw9vgIwbY5ZLPEg8adPpfsFyZteNiR4'}],\n",
       "   'enabled': False},\n",
       "  'all_awardings': [],\n",
       "  'media_only': False,\n",
       "  'can_gild': False,\n",
       "  'spoiler': False,\n",
       "  'locked': False,\n",
       "  'author_flair_text': None,\n",
       "  'visited': False,\n",
       "  'num_reports': None,\n",
       "  'distinguished': None,\n",
       "  'subreddit_id': 't5_2qi0i',\n",
       "  'mod_reason_by': None,\n",
       "  'removal_reason': None,\n",
       "  'link_flair_background_color': '',\n",
       "  'id': 'awb0vh',\n",
       "  'is_robot_indexable': True,\n",
       "  'report_reasons': None,\n",
       "  'author': 'jswanson40',\n",
       "  'num_crossposts': 0,\n",
       "  'num_comments': 31,\n",
       "  'send_replies': True,\n",
       "  'whitelist_status': 'all_ads',\n",
       "  'contest_mode': False,\n",
       "  'mod_reports': [],\n",
       "  'author_patreon_flair': False,\n",
       "  'author_flair_text_color': None,\n",
       "  'permalink': '/r/weather/comments/awb0vh/landicane_over_us_in_north_dakota_back_in/',\n",
       "  'parent_whitelist_status': 'all_ads',\n",
       "  'stickied': False,\n",
       "  'url': 'https://v.redd.it/7c4357xqalj21',\n",
       "  'subreddit_subscribers': 50830,\n",
       "  'created_utc': 1551481963.0,\n",
       "  'media': {'reddit_video': {'fallback_url': 'https://v.redd.it/7c4357xqalj21/DASH_1080?source=fallback',\n",
       "    'height': 1080,\n",
       "    'width': 608,\n",
       "    'scrubber_media_url': 'https://v.redd.it/7c4357xqalj21/DASH_240',\n",
       "    'dash_url': 'https://v.redd.it/7c4357xqalj21/DASHPlaylist.mpd',\n",
       "    'duration': 7,\n",
       "    'hls_url': 'https://v.redd.it/7c4357xqalj21/HLSPlaylist.m3u8',\n",
       "    'is_gif': True,\n",
       "    'transcoding_status': 'completed'}},\n",
       "  'is_video': True}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dict['data']['children'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can finally use Pandas to put all that data together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ups</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>subreddit_name_prefixed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Storm chasers are paying tribute to Bill Paxto...</td>\n",
       "      <td>1830</td>\n",
       "      <td>70</td>\n",
       "      <td>r/weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm from Colima. I'm gonna try to post whateve...</td>\n",
       "      <td>1089</td>\n",
       "      <td>724</td>\n",
       "      <td>r/weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Weather Channel has the perfect response f...</td>\n",
       "      <td>918</td>\n",
       "      <td>78</td>\n",
       "      <td>r/weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From someone in an area that never has tornado...</td>\n",
       "      <td>827</td>\n",
       "      <td>651</td>\n",
       "      <td>r/weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Heard the sirens. Stepped out of my office. Th...</td>\n",
       "      <td>730</td>\n",
       "      <td>83</td>\n",
       "      <td>r/weather</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title   ups  num_comments  \\\n",
       "0  Storm chasers are paying tribute to Bill Paxto...  1830            70   \n",
       "1  I'm from Colima. I'm gonna try to post whateve...  1089           724   \n",
       "2  The Weather Channel has the perfect response f...   918            78   \n",
       "3  From someone in an area that never has tornado...   827           651   \n",
       "4  Heard the sirens. Stepped out of my office. Th...   730            83   \n",
       "\n",
       "  subreddit_name_prefixed  \n",
       "0               r/weather  \n",
       "1               r/weather  \n",
       "2               r/weather  \n",
       "3               r/weather  \n",
       "4               r/weather  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(posts)\n",
    "df.head()\n",
    "df = df[['title', 'ups', 'num_comments', 'subreddit_name_prefixed']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIs with Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it really gets that good! Wrappers are sometimes created to make your life easier. In this next example I am pulling data from the Quandl API. Quandly is a financial data company. https://www.quandl.com/tools/api\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's super easy to use! Just sign up for an API key and the website will direct you exactly what you need to do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might fist need to Pip Install quandl\n",
    "import quandl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am hiding my API Key, but it really is that easy! Just follow the site and you can pull data just like that!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: The bottom cell won't execute on your computer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bullish</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Bearish</th>\n",
       "      <th>Total</th>\n",
       "      <th>Bullish 8-Week Mov Avg</th>\n",
       "      <th>Bull-Bear Spread</th>\n",
       "      <th>Bullish Average</th>\n",
       "      <th>Bullish Average + St. Dev</th>\n",
       "      <th>Bullish Average - St. Dev</th>\n",
       "      <th>S&amp;P 500 Weekly High</th>\n",
       "      <th>S&amp;P 500 Weekly Low</th>\n",
       "      <th>S&amp;P 500 Weekly Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-08-09</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.326019</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.337165</td>\n",
       "      <td>0.053291</td>\n",
       "      <td>0.381696</td>\n",
       "      <td>0.482914</td>\n",
       "      <td>0.280477</td>\n",
       "      <td>2863.43</td>\n",
       "      <td>2796.34</td>\n",
       "      <td>2857.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-16</th>\n",
       "      <td>0.361702</td>\n",
       "      <td>0.347518</td>\n",
       "      <td>0.290780</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333979</td>\n",
       "      <td>0.070922</td>\n",
       "      <td>0.381696</td>\n",
       "      <td>0.482914</td>\n",
       "      <td>0.280477</td>\n",
       "      <td>2862.48</td>\n",
       "      <td>2802.49</td>\n",
       "      <td>2818.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-23</th>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.344729</td>\n",
       "      <td>0.270655</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.346496</td>\n",
       "      <td>0.113960</td>\n",
       "      <td>0.381696</td>\n",
       "      <td>0.482914</td>\n",
       "      <td>0.280477</td>\n",
       "      <td>2873.23</td>\n",
       "      <td>2802.49</td>\n",
       "      <td>2861.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-30</th>\n",
       "      <td>0.434959</td>\n",
       "      <td>0.321138</td>\n",
       "      <td>0.243902</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.366047</td>\n",
       "      <td>0.191057</td>\n",
       "      <td>0.381696</td>\n",
       "      <td>0.482914</td>\n",
       "      <td>0.280477</td>\n",
       "      <td>2916.50</td>\n",
       "      <td>2854.03</td>\n",
       "      <td>2914.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-06</th>\n",
       "      <td>0.422222</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.262963</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.365011</td>\n",
       "      <td>0.159259</td>\n",
       "      <td>0.381696</td>\n",
       "      <td>0.482914</td>\n",
       "      <td>0.280477</td>\n",
       "      <td>2916.50</td>\n",
       "      <td>2876.92</td>\n",
       "      <td>2888.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-13</th>\n",
       "      <td>0.320896</td>\n",
       "      <td>0.350746</td>\n",
       "      <td>0.328358</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.361794</td>\n",
       "      <td>-0.007462</td>\n",
       "      <td>0.381696</td>\n",
       "      <td>0.482914</td>\n",
       "      <td>0.280477</td>\n",
       "      <td>2894.65</td>\n",
       "      <td>2864.12</td>\n",
       "      <td>2888.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-20</th>\n",
       "      <td>0.320423</td>\n",
       "      <td>0.359155</td>\n",
       "      <td>0.320423</td>\n",
       "      <td>1.000001</td>\n",
       "      <td>0.362445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381696</td>\n",
       "      <td>0.482914</td>\n",
       "      <td>0.280477</td>\n",
       "      <td>2912.36</td>\n",
       "      <td>2879.20</td>\n",
       "      <td>2907.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-27</th>\n",
       "      <td>0.362205</td>\n",
       "      <td>0.326772</td>\n",
       "      <td>0.311024</td>\n",
       "      <td>1.000001</td>\n",
       "      <td>0.371332</td>\n",
       "      <td>0.051181</td>\n",
       "      <td>0.381696</td>\n",
       "      <td>0.482914</td>\n",
       "      <td>0.280477</td>\n",
       "      <td>2940.91</td>\n",
       "      <td>2903.28</td>\n",
       "      <td>2905.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-04</th>\n",
       "      <td>0.456621</td>\n",
       "      <td>0.292237</td>\n",
       "      <td>0.251142</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.382955</td>\n",
       "      <td>0.205479</td>\n",
       "      <td>0.381696</td>\n",
       "      <td>0.482914</td>\n",
       "      <td>0.280477</td>\n",
       "      <td>2939.86</td>\n",
       "      <td>2903.28</td>\n",
       "      <td>2925.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-11</th>\n",
       "      <td>0.306061</td>\n",
       "      <td>0.339394</td>\n",
       "      <td>0.354545</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.376000</td>\n",
       "      <td>-0.048484</td>\n",
       "      <td>0.381696</td>\n",
       "      <td>0.482914</td>\n",
       "      <td>0.280477</td>\n",
       "      <td>2939.86</td>\n",
       "      <td>2784.86</td>\n",
       "      <td>2785.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Bullish   Neutral   Bearish     Total  Bullish 8-Week Mov Avg  \\\n",
       "Date                                                                         \n",
       "2018-08-09  0.363636  0.326019  0.310345  1.000000                0.337165   \n",
       "2018-08-16  0.361702  0.347518  0.290780  1.000000                0.333979   \n",
       "2018-08-23  0.384615  0.344729  0.270655  0.999999                0.346496   \n",
       "2018-08-30  0.434959  0.321138  0.243902  0.999999                0.366047   \n",
       "2018-09-06  0.422222  0.314815  0.262963  1.000000                0.365011   \n",
       "2018-09-13  0.320896  0.350746  0.328358  1.000000                0.361794   \n",
       "2018-09-20  0.320423  0.359155  0.320423  1.000001                0.362445   \n",
       "2018-09-27  0.362205  0.326772  0.311024  1.000001                0.371332   \n",
       "2018-10-04  0.456621  0.292237  0.251142  1.000000                0.382955   \n",
       "2018-10-11  0.306061  0.339394  0.354545  1.000000                0.376000   \n",
       "\n",
       "            Bull-Bear Spread  Bullish Average  Bullish Average + St. Dev  \\\n",
       "Date                                                                       \n",
       "2018-08-09          0.053291         0.381696                   0.482914   \n",
       "2018-08-16          0.070922         0.381696                   0.482914   \n",
       "2018-08-23          0.113960         0.381696                   0.482914   \n",
       "2018-08-30          0.191057         0.381696                   0.482914   \n",
       "2018-09-06          0.159259         0.381696                   0.482914   \n",
       "2018-09-13         -0.007462         0.381696                   0.482914   \n",
       "2018-09-20          0.000000         0.381696                   0.482914   \n",
       "2018-09-27          0.051181         0.381696                   0.482914   \n",
       "2018-10-04          0.205479         0.381696                   0.482914   \n",
       "2018-10-11         -0.048484         0.381696                   0.482914   \n",
       "\n",
       "            Bullish Average - St. Dev  S&P 500 Weekly High  \\\n",
       "Date                                                         \n",
       "2018-08-09                   0.280477              2863.43   \n",
       "2018-08-16                   0.280477              2862.48   \n",
       "2018-08-23                   0.280477              2873.23   \n",
       "2018-08-30                   0.280477              2916.50   \n",
       "2018-09-06                   0.280477              2916.50   \n",
       "2018-09-13                   0.280477              2894.65   \n",
       "2018-09-20                   0.280477              2912.36   \n",
       "2018-09-27                   0.280477              2940.91   \n",
       "2018-10-04                   0.280477              2939.86   \n",
       "2018-10-11                   0.280477              2939.86   \n",
       "\n",
       "            S&P 500 Weekly Low  S&P 500 Weekly Close  \n",
       "Date                                                  \n",
       "2018-08-09             2796.34               2857.70  \n",
       "2018-08-16             2802.49               2818.37  \n",
       "2018-08-23             2802.49               2861.82  \n",
       "2018-08-30             2854.03               2914.04  \n",
       "2018-09-06             2876.92               2888.60  \n",
       "2018-09-13             2864.12               2888.92  \n",
       "2018-09-20             2879.20               2907.95  \n",
       "2018-09-27             2903.28               2905.97  \n",
       "2018-10-04             2903.28               2925.51  \n",
       "2018-10-11             2784.86               2785.68  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quandl.ApiConfig.api_key = 'Hidden API KEY'\n",
    "SPY=quandl.get('AAII/AAII_SENTIMENT', start_date='2010-01-01', end_date='2018-10-13')\n",
    "SPY.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
